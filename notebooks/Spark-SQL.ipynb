{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pyspark environment\n",
    "# Use the conda environment in this path: /Users/rmontecino/anaconda3/envs/pyspark-env\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = '/Users/rmontecino/anaconda3/envs/pyspark-env/lib/python3.12/site-packages/pyspark'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark and initialize SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/07 21:23:19 WARN Utils: Your hostname, Ricardos-MacBook-Pro-CleverIT.local resolves to a loopback address: 127.0.0.1; using 192.168.1.86 instead (on interface en0)\n",
      "24/08/07 21:23:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/07 21:23:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/07 21:23:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/08/07 21:23:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------+---+------+------+\n",
      "|          name|age|gender|salary|\n",
      "+--------------+---+------+------+\n",
      "|      John Doe| 30|  Male| 50000|\n",
      "|    Jane Smith| 25|Female| 45000|\n",
      "| David Johnson| 35|  Male| 60000|\n",
      "|   Emily Davis| 28|Female| 52000|\n",
      "|Michael Wilson| 40|  Male| 75000|\n",
      "+--------------+---+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "data_file = '../data/persons.csv'\n",
    "persons_df = spark.read.csv(data_file, header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "persons_df.printSchema()\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "persons_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the DataFrame as a Temporary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table\n",
    "persons_df.createOrReplaceTempView('persons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform SQL-like Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n",
      "+----------+\n",
      "|avg_salary|\n",
      "+----------+\n",
      "|   57200.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all rows where age is greater than 25\n",
    "query = 'SELECT * FROM persons WHERE age > 25'\n",
    "persons_df_greater_than_25 = spark.sql(query)\n",
    "persons_df_greater_than_25.show()\n",
    "\n",
    "# Compute the average salary of persons\n",
    "query = 'SELECT AVG(salary) AS avg_salary FROM persons'\n",
    "avg_salary = spark.sql(query)\n",
    "avg_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing temporary views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temporary view persons exists\n"
     ]
    }
   ],
   "source": [
    "# Check if a temporary view exists\n",
    "if spark.catalog._jcatalog.tableExists('persons'):\n",
    "    print('The temporary view persons exists')\n",
    "\n",
    "# Drop the temporary view\n",
    "spark.catalog.dropTempView('persons')\n",
    "\n",
    "# Check if a temporary view exists\n",
    "if spark.catalog._jcatalog.tableExists('persons'):\n",
    "    print('The temporary view persons exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1| John|\n",
      "|  2| Jane|\n",
      "|  3|Alice|\n",
      "+---+-----+\n",
      "\n",
      "+---+------+-----------+\n",
      "| id|salary| department|\n",
      "+---+------+-----------+\n",
      "|  1|  1000|         HR|\n",
      "|  2|  1500|Engineering|\n",
      "|  3|  1200|  Marketing|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two DataFrames\n",
    "# The first DataFrame contains employee data with columns: id, name\n",
    "# The second DataFrame contains salary data with columns: id, salary, department\n",
    "data1 = [(1, 'John'), (2, 'Jane'), (3, 'Alice')]\n",
    "data2 = [(1, 1000, 'HR'), (2, 1500, 'Engineering'), (3, 1200, 'Marketing')]\n",
    "columns1 = ['id', 'name']\n",
    "columns2 = ['id', 'salary', 'department']\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Show the first DataFrame\n",
    "df1.show()\n",
    "\n",
    "# Show the second DataFrame\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary views\n",
    "df1.createOrReplaceTempView('employees')\n",
    "df2.createOrReplaceTempView('salaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|salary|\n",
      "+----+------+\n",
      "|Jane|  1500|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subquery to find employees with salaries above average\n",
    "query = '''\n",
    "SELECT e.name, s.salary\n",
    "FROM employees e\n",
    "JOIN salaries s\n",
    "ON e.id = s.id\n",
    "WHERE s.salary > (SELECT AVG(salary) FROM salaries)\n",
    "'''\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
